{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71650bd-6c91-444b-b628-df2d07e463cf",
   "metadata": {},
   "source": [
    "# 02 Statistical Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc418e5-bb6d-4110-966c-4f4fce0f05e5",
   "metadata": {},
   "source": [
    "> \"Not everything that can be counted counts, and not everything that counts can be counted.\" ~ Albert Einstein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19734feb-0cb3-4d2c-ab6e-41b8c0299fc9",
   "metadata": {},
   "source": [
    "![img](https://images.squarespace-cdn.com/content/v1/55b9303be4b0b1a374b0842c/1438564277310-4GRJ1VBUMAI8PJPPJRFC/image-asset.png?format=2500w)\n",
    "\n",
    "**Source:** Virginia W. Mason, NG Staff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214f35a-b9c0-4ed3-8fdb-d9f510ad285f",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5bf467-9f01-47d4-b7d7-7fa5d31170c8",
   "metadata": {},
   "source": [
    "By the end of this notebook you will have\n",
    "- a better understanding of the differences between machine learning and statistics,\n",
    "- learned how to run regressions over partitions large datasets and groups of datasets,\n",
    "- learned how to take a model and create a quick-and-dirty application to showcase your work,\n",
    "- a better understanding of what spatial regression is and how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5938a1-840e-413e-a63e-4b5e067b047d",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c3490-9183-4509-ad7e-ea5f3a0c1c81",
   "metadata": {},
   "source": [
    "1. Statistics vs ML\n",
    "2. Hypothesis Testing Refresher\n",
    "3. What do You Mean(s)?\n",
    "4. Linear Regression\n",
    "5. Regression Everywhere\n",
    "    - ML\n",
    "    - SM\n",
    "    - In Pieces\n",
    "    - Evaluate it in an application\n",
    "6. Spatial Regression\n",
    "7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13a3b1-f8a6-48ec-8a1a-5b063650100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd, numpy as np, os\n",
    "from os.path import join\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import holoviews as hv\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary import summary\n",
    "from statsmodels.iolib.summary2 import summary_model, summary_col\n",
    "import dask_ml.metrics as metrics\n",
    "from scipy import stats\n",
    "import panel as pn\n",
    "\n",
    "hv.extension('bokeh')\n",
    "pn.extension()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029f565-3e62-4ce6-bf5e-b3e9ad23a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = join('..', 'data', 'final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f3ec1-ccee-4cec-b3c8-c19b3abd74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(data_path)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4249ad-7046-44ae-8a83-143ef113a508",
   "metadata": {},
   "source": [
    "If at any point your computer starts getting too slow to follow along, you can reduce the sample size using the cell below and continue with the process again with a smaller-in-size version. The `frac=` parameter takes the persentage of the dataset that you wish to use. The `.persist()` method makes sure you don't have to wait for that computation to happen again eveytime you run a function by persisting the state of that version of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9cd8d-4779-480f-8c16-92cddb03bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    ddf = ddf.sample(frac=0.3).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98210c-cca6-4af3-9b51-b0a66ca69b77",
   "metadata": {},
   "source": [
    "## 1. Statistics vs ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6739129-1d42-4b30-a50a-6c6178281ae5",
   "metadata": {},
   "source": [
    "> \"Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns.\" ~ Danilo Bzdok, Naomi Altman & Martin Krzywinski \n",
    "\n",
    "\n",
    "> \"Statistics is the mathematical study of data. You cannot do statistics unless you have data. A statistical model is a model for the data that is used either to infer something about the relationships within the data or to create a model that is able to predict future values. Often, these two go hand-in-hand.\"\n",
    "\n",
    "Some Differences:\n",
    "\n",
    "- SMs can make predictions, but their fortÃ© is not necessarily their accuracy.\n",
    "- MLMs optimize for predictive accuracy, which can lead to less interpretability.\n",
    "- SMs use the entire data to find the best fit and make an inference/test a hypothesis.\n",
    "- MLMs are trained on a subset of the data and validated with another unseen one to the model.\n",
    "- SMs seek to model/characterise the relationship between the data (independent variables) and the outcome (dependant) variable.\n",
    "- MLMs seek to optimize the prediction of future data points.\n",
    "- SMs can also be used to prdict.\n",
    "- MLMs are not exactly a characterization of the target variable given the data. Good predictions (ðŸ’¸ðŸ’°ðŸ’¸) > make sense (ðŸ’°).\n",
    "- Regressors and a Linear Regression are not the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcea6b7-5fa6-4d00-9fc7-2990acb4a7f9",
   "metadata": {},
   "source": [
    "## 2. Hypothesis Testing Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc28f8-a8d3-46e1-9d1a-0f946ca617d5",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/null_hypothesis.png\" alt=\"null h\" width=\"300\"/>  \n",
    "\n",
    "**Source:** https://xkcd.com/892/\n",
    "\n",
    "As data professionals, we will often want to test a variety of questions with whatever data we have on hand to improve and enhance the decision-making process of our organisations. We will also want to know whether these questions hold true against the evience we currently have, and to do this, we turn to the inferential side of statistics and compare a variety of methods suitable for different data types.\n",
    "\n",
    "Since we assume the data we have comes from some type of random process and that most often than not, we don't hold all of it, when we answer a hypothesis test we do so in terms of a sample of the population from which the data was taken. Having one or the other will tend to dictate, in some cases, which method you use. Let's define what a hypothesis is in a general way.\n",
    "\n",
    "> \"A hypothesis is a question, premise, claim or idea that we want to test using data\"\n",
    "\n",
    "The most basic elements of a hypothesis are\n",
    "- $H_{0}$ --> (pronounced H of not) is the Null Hypothesis or **the idea that we want to disprove.** You can also think of this as the status quo or what has already been accepted by the majority but not proven false yet.\n",
    "- $H_{A}$ --> (pronounced H of A) is the alternative hypothesis we want to test and hopefully prove is closer to the truth than the null hypothesis. It is also called the research hypothesis.\n",
    "- Reject $H_{0}$ --> this is the conclusion we reach we get when we do find evidence against the status quo, accepted view or idea.\n",
    "- Fail to reject $H_{0}$ --> this is the result we get when we are unable to disproved the accepted idea with our results.\n",
    "- Level of Confidence --> usually set to 95%. Is our degree of confidence in our decision. For example, if we are rejecting a null hypothesis we could say, \"I am 95% confident that rejecting the null hypothesis is a valid conclusion.\"\n",
    "- $\\alpha$ --> Is the level of significance of our decision. This is calculated by subtracting 95% from 1 (e.g. $\\alpha = 1 - 95%$ and it is the explicit threshold by which the probability we get from our result needs to be at or below to disprove the null hypothesis. In other words, the probability estimate generated from our hypothesis test has to be below 0.05 for it to be meaningful to us.\n",
    "- $p-value$ --> Is a probability and a result we get from a hypothesis test. If this number is below $\\alpha$, then it allows us to disprove the null hypothesis. In other words,\n",
    "    - if $p-value > \\alpha$ we **fail to reject** the Null Hypothesis\n",
    "    - if $p-value < \\alpha$ we **reject** the Null Hypothesis. Another way of wording this: There is a 5% chance that two identical distributions would have produce the results we are observing.\n",
    "- Statistical significance --> the measure or place where we draw the line to be able to say that our results are strong enough to disprove what is currently accepted.\n",
    "\n",
    "Formal definition of the p-value\n",
    "> Probability of obtaining a sample more extreme than the ones observed in your data, assuming that the Null Hypothesis is true.\n",
    "\n",
    "Think about the last two elements above in terms of the guinness world records, until someone comes and beats a record in that book, we cannot disprove him/her/they/them as the true record holder. The case is the same for hypothesis testing.\n",
    "\n",
    "\n",
    "### Sides of a hypothesis\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/862/1*VXxdieFiYCgR6v7nUaq01g.jpeg\" alt=\"h-sides\" width=\"700\"/> \n",
    "\n",
    "\n",
    "### Testing options given your data\n",
    "\n",
    "| Comparison | Data you have | Test Available |\n",
    "|-----|---------|----|\n",
    "| 1 Sample vs Known Population | categorical | Binomial Test |\n",
    "| 1 Sample vs Known Population | numerical | 1 Sample t-test |\n",
    "| 2 Samples | categorical | Chi-Square |\n",
    "| 2 Samples | numerical | 2 Sample t-test |\n",
    "| More than 2 | categorical | Chi-Square |\n",
    "| More than 2 | numerical | ANOVA and/or Tukey |\n",
    "\n",
    "\n",
    "Another way of reframing hypothesis testing is by asking\n",
    "\n",
    "> What is the probability that the difference I am observing is due to chance?\n",
    "\n",
    "Lastly, remember that the goal of hypothesis testing is to disprove the Null Hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54becd3-8a3f-428c-8b06-b8173c4913db",
   "metadata": {},
   "source": [
    "## 3. What do You Mean(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3f6a3-4af6-43e5-935a-8fbf59fc9f8c",
   "metadata": {},
   "source": [
    "Now that we have a better idea of what hypothesis testing is, let's use one of the well-known statistical tests out there, the t-test, to compare the means of two important price distributions in our datasets, those of super hosts and regulr hosts.\n",
    "\n",
    "**What is a t-test?**\n",
    "\n",
    "> \"A t-test is a statistical test that is used to compare the means of two groups. It is often used in hypothesis testing to determine whether a process or treatment actually has an effect on the population of interest, or whether two groups are different from one another.\" ~ [Scribbr](https://www.scribbr.com/statistics/t-test/)\n",
    "\n",
    "**There are 2 important types of t-tests**\n",
    "- Student's t-test - the variance of both means **IS** the same\n",
    "- Welch's t-test - the variance of both means is **Not** the same\n",
    "\n",
    "What we want to make sure is that if there is a difference between the prices of these two categories, that that difference is not due to chance.\n",
    "\n",
    "> I the average price charged by a super host the same as that charged by a regular host?\n",
    "\n",
    "1. Step 1, state the hypotheses.\n",
    "    - $H_{0}$ - The average price of the listing of a super host is the same as that of a regular host.\n",
    "    - $H_{A}$ - The average price of a listing is differrent between super hosts and regular hosts?\n",
    "2. Step 2, check whether the variances are equal using the Brown-Forsythe test\n",
    "    - $H_{0}$ - the variances are equal\n",
    "    - $H_{A}$ - the variances are different\n",
    "    - If enough evidence exists and the variances are not equal, proceed with the Welch's t-test\n",
    "3. Step 3, pick the appropriate Statistical test, Student's t-test or Welch's t-test\n",
    "4. Analyze the results\n",
    "\n",
    "Brown-Forsythe Test Formula\n",
    "\n",
    "$F = \\frac{(N - p)}{(p - 1)} \\frac{\\sum^{p}_{j=1} n_{j} (\\overline{z}_{.j} - \\overline{z}_{..})^2}{\\sum^{p}_{j=1} \\sum^{n_j}_{i=1} (\\overline{z}_{ij} - \\overline{z}_{.j})^2}$\n",
    "\n",
    "Where: $\\overline{z}_{ij} = | y_ij - \\overline{y}_{j}|$\n",
    "\n",
    "- $N$ is the count of the observation\n",
    "- $p$ is the number of groups\n",
    "- $n_j$ is the number of observations in group $j$\n",
    "- $\\overline{y}_{j}$ is the median of group j\n",
    "- $\\overline{z}_{.j}$ is the mean of group j\n",
    "- $\\overline{z}_{..}$ is the mean of all $z_ij$\n",
    "\n",
    "Let's first filter our the top and bottom 1% of our distribution and then proceed to calculate the first part of the equation, also called the degrees of freedom.\n",
    "\n",
    "$\\frac{(N - p)}{(p - 1)}$\n",
    "\n",
    "**Note:** This example is an adapted version of the one in the excellent book titles, \"Data Science with Python and Dask\" by Jesse C. Daniel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f9af9-2e7e-4320-bc30-2f33e00a236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_outliers = ddf.price < ddf.price.quantile(0.99)\n",
    "lower_outliers = ddf.price > ddf.price.quantile(0.01)\n",
    "\n",
    "ddf1 = ddf[upper_outliers & lower_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cc85a-f823-40e0-9d00-d606ded60207",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    print(F\"Remember how skeewed our data is {ddf1.price.skew().compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19e923-5627-4aa7-81c9-d0025a6e930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the balance between both\n",
    "ddf1.host_is_superhost.value_counts(normalize=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c4a79-6950-4c9e-827e-08d9be3441fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    N = ddf1.price.count().compute() # how many prices?\n",
    "    p = ddf1.host_is_superhost.nunique().compute() # how many groups?\n",
    "\n",
    "brown_left = (N - p) / (p - 1)\n",
    "print(f\"This is the left hand side of our equation --> {brown_left}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df4c00-caa5-4ecf-a2fb-0694e936af15",
   "metadata": {},
   "source": [
    "We now need the right hand side of the equation so let's split the data between the 2 groups and calculate the bottom part starting with the median price of each group.\n",
    "\n",
    "$\\frac{\\sum^{p}_{j=1} n_{j} (\\overline{z}_{.j} - \\overline{z}_{..})^2}{\\sum^{p}_{j=1} \\sum^{n_j}_{i=1} (\\overline{z}_{ij} - \\overline{z}_{.j})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af89e7-b60a-4f65-bd23-650fe349699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    superh = ddf1[ddf1.host_is_superhost == 't']\n",
    "    regularh = ddf1[ddf1.host_is_superhost == 'f']\n",
    "    \n",
    "    med_super = superh['price'].quantile(0.5).compute()\n",
    "    med_regul = regularh['price'].quantile(0.5).compute()\n",
    "    \n",
    "print(f\"This is the median price for super hosts --> {med_super}\")\n",
    "print(f\"This is the median price for regular hosts --> {med_regul}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f61dde-d963-49bd-97fc-0bbd3cf88d5a",
   "metadata": {},
   "source": [
    "We now need to subtract the median from each of the prices in both groups with a function that takes advantage of broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a96e96-227c-4696-8043-ef83eb1b0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_dev_from_med(row):\n",
    "    if row['host_is_superhost'] == 't':\n",
    "        return abs(row['price'] - med_super)\n",
    "    else:\n",
    "        return abs(row['price'] - med_regul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02303fc1-1bbd-4da9-bb0e-908448fe79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_differences = ddf1.apply(abs_dev_from_med, axis=1, meta=float)\n",
    "ddf_stg1 = ddf1.assign(median_differences=median_differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13664aef-7b77-4e01-85f2-bdbab44d0b87",
   "metadata": {},
   "source": [
    "We need to calculate the mean of the `median_differences` differences of both groups and then subtract it from the median_differences of each group and square it. We will do so with a similar function to the one above and reassign the results to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb181ff-283e-4a9c-b749-d442402ea0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    group_means = ddf_stg1.groupby('host_is_superhost')['median_differences'].mean().compute()\n",
    "\n",
    "group_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93cc88-08ec-4eb8-a3b1-9a5b04b30020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean_var(row):\n",
    "    if row['host_is_superhost'] == 't':\n",
    "        return (row['median_differences'] - group_means['t']) ** 2\n",
    "    else:\n",
    "        return (row['median_differences'] - group_means['f']) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fabc74-7dc3-43a0-8349-e5daccdf1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the results is the groups mean variances\n",
    "group_mvars = ddf_stg1.apply(group_mean_var, axis=1, meta=float)\n",
    "ddf_stg2 = ddf_stg1.assign(group_mvars=group_mvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ab9b0-b112-483d-904d-f831ee55d915",
   "metadata": {},
   "source": [
    "Sum up the group mean variances to finish up the denominator part of our equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67ee1a-a237-4403-943b-1fafe52e7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    brown_denom = ddf_stg2['group_mvars'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51a04e-16e2-4857-9003-34c7ff08297d",
   "metadata": {},
   "source": [
    "Lastly, the numerator can be gathered first by calculating the mean of the median differrences, or the column without any grouping, and second, by counting and summing the chunks, then diving the sum by the count to get the mean we will subtract from the grand means. Lastly, we square the result.\n",
    "\n",
    "We can achieve this calculation with dask's `Aggregation` function, which takes 4 parts:\n",
    "1. The name of the function\n",
    "2. The initial function used on each partition\n",
    "3. An aggregation function for the result of each partition\n",
    "4. An optional function to transform the result from step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8dc014-59ee-4cdb-889e-035c1b9a6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    grand_means = ddf_stg2['median_differences'].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c555d3-06c3-4d74-ab99-affbcfc8f5e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf_stg2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473cd69-bdf2-45c5-ae57-2af0a891ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_agg = dd.Aggregation(\n",
    "    \"Brown_Aggregation\",\n",
    "    lambda chunk: (chunk.count(), chunk.sum()), # count obs in a chunk and also sum them up\n",
    "    lambda chunk_count, chunk_sum: (chunk_count.sum(), chunk_sum.sum()), # add up the chunks counts and the chunks sums\n",
    "    # divide the full sum by the full count, subtract the mean, and square the result\n",
    "    lambda group_count, group_sum: group_count * (((group_sum / group_count) - grand_means) ** 2) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dd10a-eb4f-4afb-95ae-13d8f46c778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    group_variances = ddf_stg2.groupby('host_is_superhost').agg({'median_differences': brown_agg}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21e1b7-3069-47cd-82f7-7755b820ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_numerator = group_variances.sum().item()\n",
    "brown_numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1588fe7-ee92-4b89-a526-4f2b896021ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_stat = brown_left * (brown_numerator / brown_denom)\n",
    "F_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79727e-05f2-49dd-a1b3-50d44d58f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_critical = stats.f.ppf(q=1-0.5, dfn=p-1, dfd=N-p)\n",
    "F_critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16dd8f-5baf-4463-a117-22295cae45b6",
   "metadata": {},
   "source": [
    "Here's what we got for the Brown-Forsythe test.\n",
    "\n",
    "\n",
    "If the F Statistic (our test result) is greater than the F-Critical, a threshold from the F distribution, we can say that there is sufficient evidence to reject the null hypothesis. Hence, the variances are different and we can proceed with the Welch's t-test to check whether the prices of both groups are really different.\n",
    "\n",
    "The above F-Critical function comes from the `scipy.stats` module and it takes 2 values, the degrees of freedom of both the groups and the observations as we calculated in the first part, and then 1 minus the percentage of error we are willing to tolerate (5%), aka our confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497775f-191f-4fce-bf44-0918e16b5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    sup = superh['price'].values.compute()\n",
    "    reg = regularh['price'].values.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94626618-e136-4eb5-93a4-eff36ebcf745",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.stats.ttest_ind(sup, reg, equal_var=False).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0062d-0127-469e-a442-f3f8d22fabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(sup, reg, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c3cea-ffce-4a9a-9fbe-e8760c802057",
   "metadata": {},
   "source": [
    "Here we pay attention to the p-value. If the result we get is less than the threshold we have chosen, 0.05, we can reject the null hypothesis, otherwise, we fail to reject it.\n",
    "\n",
    "There appears to be a significant difference between the prices of super hosts versus those of regular hosts and we can go ahead and reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaca705-656d-488e-9580-120ec2462e77",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Imagine we believe the time of response has no effect on the price of a listing, meaning, whether a host takes a few hours or days to reponds don't affect the price of a listing. Following the receipe from above,\n",
    "1. Create a boolean variable for hosts that respond the same day and those that don't.\n",
    "2. Test whether both groups have equal variances.\n",
    "3. Test whether both groups charge on average the same for their listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf99f15-49e5-4988-bc1a-ff0ca8bbec63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be959e-8736-43e1-93be-a161ecff9205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8f74a-c29e-4bdd-9483-263b2d46aa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc0364-400f-49f3-bf08-baf01abde186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61014a34-28f9-41c8-a908-4e663b0a162c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6beba656-4286-4abb-864f-2e741db1a19b",
   "metadata": {},
   "source": [
    "## 3. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7c167-3d20-45e2-b482-5466f901f363",
   "metadata": {},
   "source": [
    "The Goal of a Regression is to search for associations between a target variable and one or many variables. For example, determining the price of a house (what we want to predict) might only be possible with additional information (what will help us make a prediction) such as # of bathrooms, # of bedrooms, # garage, etc...\n",
    "\n",
    "A regression is a type of linear model with which we can quantify the relationships in our data and, at the same time, try to determine how reliable such relationship is. A linear model usually looks as follows,\n",
    "\n",
    "$y = a*x + b$\n",
    "\n",
    "- $a$ - is the slope of the line\n",
    "- $b$ - y intercept is where the line crosses the y-axis\n",
    "- $y$ - is what we are trying to predict\n",
    "- $x$ - is what we are using to predict\n",
    "\n",
    "We are interested in finding the optimal values or $a$ and $b$ which are also called parameters. \n",
    "\n",
    "You might also be wondering, which line are we talking about? The line of best fit is a line with predicted values that run through our data points as closely as possible to the center or where the data is most concentrated at. This means that the values in such a predictive line are not necessarily perfect predictors but rather the best predictors given the data, which in turn means that there will be a difference between the actual data and the predictions and these are called the errors. These errors, the differences between a predicted value and a real one, are also called residuals. Our goal is often to minimize the square distances between the observed values and the line of best fit. See the image below for an example of a line of best fit.\n",
    "\n",
    "![line](https://images.saymedia-content.com/.image/t_share/MTc0MjM1NjgwNzExNzgwMjIw/how-to-create-a-simple-linear-regression-equation.png)\n",
    "\n",
    "Nomeclature and definitions\n",
    "\n",
    "- $Y$ - The vector, array, characteristic or value that we are trying to predict. This is often called,\n",
    "    - Dependent Variable\n",
    "    - Target Variable\n",
    "    - Outcome Variable\n",
    "    - Response Variable\n",
    "- $X$ - Can be a single array or a matrix representing multiple variables. These values we use to make predictions are often called,\n",
    "    - Independent Variable(s)\n",
    "    - Features\n",
    "    - Predictor Variable(s)\n",
    "- Fitted values - the estimates obtained from the regression, aka the predicted values.\n",
    "- Coefficients - measures the strength of the relationshit between the independent variable(s) and the dependent variable as well as the sign of such relationship (i.e. positive or negative). These are also the slopes, e.g. the parameters of our model.\n",
    "- Residuals - difference between the predicted value (the line fitted) and the actual target variable.\n",
    "- $R^2$ - How much of the variation in our dependent variable is explained by the variation in the independent variable(s). This number usually goes from 0 to 1 and the way to interpret it is, \"x% of the variation in our dependent variable is explained by the variation in our independent variable(s)\".\n",
    "- Adjusted $R^2$ - scaled version of $R^2$ by the parameters.\n",
    "- Sum of Square Residuals - The residuals are the differences between the real data and the predicted line that best fits the data. We want this to be as close to 0 as possible.\n",
    "- Mean Square Error - is the average squared residuals, in other words, the average of the squared differences between the predicted values and the actual values. We want this number to be as small as possible.\n",
    "- p-values - are values that, given a pre-specified threshold, tell us how confident we can be that the results of our model were (or not) due to chance. We usually observe the p-value of each coefficient.\n",
    "\n",
    "\n",
    "Assumptions:  \n",
    "- The regression model is linear in the coefficients and the error term\n",
    "- The error term has a population mean of zero\n",
    "- All independent variables are uncorrelated with the error term\n",
    "- Observations of the error term are uncorrelated with each other\n",
    "- The error term has a constant variance\n",
    "- No independent variable is a perfectly linear function of other explanatory variables\n",
    "- The error term is normally distributed (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b359304-7df5-42f8-b2bc-1687ed26d31b",
   "metadata": {},
   "source": [
    "## 4. Regression Everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d79655-ea22-48a8-9b3a-6a9943005f15",
   "metadata": {},
   "source": [
    "In this section, we will be testing different approaches for predicting and modeling the price of a listing using linear regression. For our analysis, we will need a couple of variables and some better define ones so let's start by simplyfing these variables.\n",
    "\n",
    "Some of these ideas where taken from the awesome book, [\"Geographic Data Science with PySAL and the PyData Stack\" by Sergio J. Rey, Dani Arribas-Bel, and Levi J. Wolf](https://geographicdata.science/book/intro.html).\n",
    "\n",
    "I highly recommend this book if you are trying to learn more about geospatial analysis and data science in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cadf7-b402-4b0e-806e-f061cac34ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(property_type):\n",
    "    types = ['House', 'Apartment', 'Condominium', 'Townhouse']\n",
    "    if property_type in types:\n",
    "        return property_type\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85491d8-6073-4212-8757-7683191e4025",
   "metadata": {},
   "source": [
    "The function above will help us clean the property type columen before we create dummy variables with them (also called one-hot encoding in machine learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48652a00-c852-4438-be71-25429f1d8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpl_prop = ddf1['property_type'].apply(simplify, meta=('property_type', 'object')).to_frame() # note that we need a dataframe and not a series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a14b4-0556-40d5-9574-3ab0463970f0",
   "metadata": {},
   "source": [
    "In order to have a better-behaved (more normal-like) targe variable, we will take the log of the price column and assign it back to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3757e72-fe2b-4432-966f-e1c7ddf1969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprice = da.log(ddf1['price'])\n",
    "logprice.name = 'log_price'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780a8f6-079f-4789-8f80-34b7c8e91553",
   "metadata": {},
   "source": [
    "Let's now transform our new simplified property column and the `room_type` columns into a dummy variable. This creates a new column for every category where the appearance of a value receives a 1 and the absence of it a 0. Before we do so we need to convert both into category types with the `Categorizer()` class from dask and then and then use `dd.get_dummies()` to get our dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3cb15-c963-4c60-82ab-672a458e68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import Categorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062bb82-51d1-42b8-bdfd-0724aa788f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    rt_cat_vars = Categorizer().fit_transform(ddf1[['room_type']])\n",
    "    pg_cat_vars = Categorizer().fit_transform(simpl_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a74a8-b84f-49c7-81a3-72d53096e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = dd.get_dummies(rt_cat_vars, prefix='rt').rename(columns=lambda x: x.replace(' ', '_'))\n",
    "pg = dd.get_dummies(pg_cat_vars, prefix='pg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7453243-a049-4c98-bc7c-630dec3f6b6f",
   "metadata": {},
   "source": [
    "Here are some of the columns we will use as our independent variables we will extract them and concatenate these and our newly created ones above to a completely new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255ae8a-d9d1-44a4-a97f-4d18b1ada43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_we_need = ['neighbourhood', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'room_type', 'property_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c06711-54e7-40e8-98b0-f3d73856a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_subset = ddf1[cols_we_need]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfde80-5b0a-43fe-9d53-196888d01a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_4_modeling = dd.concat([ddf_subset, rt, pg, logprice], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85789c4a-50e2-4b1a-bd61-85b00aa6680e",
   "metadata": {},
   "source": [
    "As a quick reminder, we have 41 partitions and each of them represents a market for Airbnb that can be selected through the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c964fc-b3ff-4292-88b2-9bad32a00389",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_4_modeling.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8872a-d7ea-4d70-98fd-ac08604500b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_4_modeling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625553c-6756-4bc5-9d7d-68879eb88737",
   "metadata": {},
   "source": [
    "### The ML Approach\n",
    "\n",
    "First, we will use dask's `LinearRegression` class from the Generalized Linear Models module to run a regressor and predict the price of a listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e337d5-1246-4352-bd75-a45ed9057928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask_ml.linear_model import LinearRegression\n",
    "from dask_glm.estimators import LinearRegression\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244daa0-b56b-418f-b647-5e35b376a225",
   "metadata": {},
   "source": [
    "We will need a constant to serve as the intercept of our model. The intercept is the value our prediction would take should all coefficients turned our to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104bb7b-7341-4f54-9d18-f9af31d8781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_4_modeling['constant'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9813015-e2e4-4bda-b579-d1d717976858",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = ['constant', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'rt_Private_room',\n",
    "                  'rt_Shared_room', 'pg_Condominium','pg_House', 'pg_Other', 'pg_Townhouse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ad747-d5ad-4925-ab52-0fd7471592a1",
   "metadata": {},
   "source": [
    "We instantiate our model with the parameters `fit_intercept=False` and `max_iter=5`. The former let's dask know that we don't need an intercept as we have already added one, and the latter says run this only 5 times. Since this model is doing a full pass over the data, to optimize our time in the tutorial, we will do these many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a86640-7fc8-4ac8-8884-7e24cb002e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression(fit_intercept=False, max_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e14966-590e-4bf8-9e6f-1658a7f7eb45",
   "metadata": {},
   "source": [
    "The next step in the ml process is to split the data into a train and test/validation set. We want our model to learn parameters based on a subset and test it with another set that it has never seen before. If we did not do this, it would be difficult for us to know wether our model has overfitted (memorized the data) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a5df8-8bf5-48a1-bd3e-5628a576ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ddf_4_modeling[variable_names], ddf_4_modeling['log_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683ab5c-8ac7-448f-86fd-cd33385e0fca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProgressBar():\n",
    "    lm.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ae41a-86c2-4f05-a277-4befb04da294",
   "metadata": {},
   "source": [
    "Let's now get the predictions for the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758fdd-98d8-407a-9f90-758f48c30a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lm.predict(X_train.values)\n",
    "y_pred_test = lm.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf1942-9bee-414c-9eaa-27cd413dcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():    \n",
    "    print(f\"MAE for the train set - > {metrics.mean_absolute_error(da.exp(y_train.values), da.exp(y_pred_train))}\")\n",
    "    print(f\"MAE for the test set - > {metrics.mean_absolute_error(da.exp(y_test.values), da.exp(y_pred_test))}\")\n",
    "    print(f\"R^2 train set is --> {1 - (((y_train.values - y_pred_train) ** 2).sum() / ((y_train.values - y_train.values.mean()) ** 2).sum()).compute()}\")\n",
    "    print(f\"R^2 test set is --> {1 - (((y_test.values - y_pred_test) ** 2).sum() / ((y_test.values - y_test.values.mean()) ** 2).sum()).compute()}\")\n",
    "    print(f\"MSLE of train set - > {metrics.mean_squared_error(y_train.values, y_pred_train)}\")\n",
    "    print(f\"MSLE of test set - > {metrics.mean_squared_error(y_test.values, y_pred_test)}\")\n",
    "    print(f\"RMSE of train set - > {da.sqrt(metrics.mean_squared_error(da.exp(y_train.values), da.exp(y_pred_train)))}\")\n",
    "    print(f\"RMSE of test set - > {da.sqrt(metrics.mean_squared_error(da.exp(y_test.values), da.exp(y_pred_test)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62636a0e-ebcb-4399-88a1-5ef030932f3e",
   "metadata": {},
   "source": [
    "Let's go over what all these metrics mean. Keep in mind, that aside from the $R^2$, the lower the value the better.\n",
    "\n",
    "- Mean Absolute Error: \"In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "- Mean Squared Error: \"In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "-  Error: \n",
    "- Root Mean Squared Error: \"represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n",
    "\n",
    "In essence, we are overfitting massively with this model, and that can be due to many factors such as the variables choosen and the locations. For example, we cant expect the listings of cape-town to add predictive power to the listings of New York and vice versa. This example is for ilustrative purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5ba5a-e55c-4e8b-92b9-a5062e63f143",
   "metadata": {},
   "source": [
    "### The SM Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76833350-21e1-4b75-9900-1dfe4bb16ddc",
   "metadata": {},
   "source": [
    "Here we will be using the statsmodel package to analyze the entire data but instead of focusing on the metrics from above, we will evaluate the model and the coefficients and how well they explain the predictive variable. We will bring down the arrays into memory first and then evaluate each market on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cb46f-6359-4da3-bed1-90d91e3bca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    lr = sm.OLS(y.values.compute(), X.values.compute()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e954f-669a-4b01-a008-9e2f767c2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1d09c-69ce-4ae6-84b3-db8429bf27c1",
   "metadata": {},
   "source": [
    "### Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e350c-5e2a-4716-bd86-d07f70371e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db5870-3b32-4487-bc97-7b2d804fb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = join('..', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea228a73-8749-4fd2-bf0f-f210959c8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lm, join(models_path, 'my_first_model.pkl'))\n",
    "joblib.dump(lr, join(models_path, 'my_second_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67dadd-9de6-49d2-bd00-30ada631715f",
   "metadata": {},
   "source": [
    "### In Pieces\n",
    "\n",
    "Now, as you might expect, not every state and not every country and, probably not every market, have the same inflation levels or the same cost for a basket of goods which means that we might benefit from doing a market level analysis rather than a dataset-wide one. The good news is that we don't need to leave dask for such an analysis but rather run the regressions in each one of the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20427f9e-448a-4430-a08c-dfb6adebae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regscores(data, X, y):\n",
    "    return data.index.unique()[0], sm.OLS(data[y].values, data[X].values).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcee13-089b-4f05-8b11-ff0d90ac089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ddf_4_modeling.map_partitions(get_regscores, X=variable_names, y='log_price', meta=tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71aba4-8d63-4873-bb1c-9796bbf0b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    models = list(models.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c7347-8a11-41d4-8989-0d431a290f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3f77b-11dc-4a6e-b007-486b0d5fda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[mkt][0], '\\n', models[mkt][1].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239d123-091d-4155-bf9e-be8221d73777",
   "metadata": {},
   "source": [
    "### Create an Application to test your models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9714a-2c34-40bd-833e-cad085985064",
   "metadata": {},
   "source": [
    "Say you want to put together a quick application and test different combinations of inputs for your models, let's do just that.\n",
    "\n",
    "- First, we will create widgets for our inputs.\n",
    "- Second, we will create a function with our models encapsulated in them.\n",
    "- Third, we will move our widgets and function into a panel and run it on the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90794c50-3d64-49b2-88b9-45c6559236b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accommodates = pn.widgets.IntSlider(name='Accommodates', start=1, end=160, step=1, value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b37fc-3427-4afa-8cf8-458e2de4c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baths = pn.widgets.FloatSlider(name='Bathrooms', start=0, end=70, step=0.5, value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a34e0-5876-451a-9b20-d8b71a914e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrooms = pn.widgets.IntSlider(name='Bedrooms', start=0, end=50, step=1, value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04658b4-edbd-4209-a79a-432b62b5408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = pn.widgets.IntSlider(name='Beds', start=0, end=170, step=1, value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df83729-a365-4437-8d50-416eb4c8bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_type = pn.widgets.Select(value='Private room', options=['Private room', \"Shared room\"], name='Type of Room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d0d81-8a36-4da9-8cde-4c78aaf6d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_type = pn.widgets.Select(value='House', options=['House', \"Condominium\", 'Townhouse', 'Other'], name='Property Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924c37d-ff4a-48a9-9fe8-4e9be1f09385",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [accommodates.param.value, baths.param.value, bedrooms.param.value, beds.param.value, room_type.param.value, pg_type.param.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9321f6-f792-46ba-a280-a7bbe0a11969",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(*parameters)\n",
    "def get_ml_prediction(accommodates, baths, bedrooms, beds, room_type, pg_type, **kwargs):\n",
    "    \n",
    "    const = 1\n",
    "    pr, sr = 0, 0\n",
    "    house, condo, townh, other = 0, 0, 0, 0\n",
    "    \n",
    "    if room_type == \"Private room\": pr += 1\n",
    "    else: sr += 1\n",
    "        \n",
    "    if pg_type == 'House': house += 1\n",
    "    elif pg_type == 'Condominium': condo += 1\n",
    "    elif pg_type == 'Townhouse': townh += 1\n",
    "    else: other += 1\n",
    "    \n",
    "    the_data = np.array((const, accommodates, baths, bedrooms, beds, pr, sr, house, condo, townh, other)).reshape(1, 11)\n",
    "    \n",
    "    num = lm.predict(the_data)\n",
    "    \n",
    "    return pn.indicators.Number(name=\"Machine Learning Prediction\", value=round(np.exp(num[0]), 3), default_color='#3B4252', \n",
    "                                font_size='60pt', title_size='50pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f1032-2da4-40db-9935-8fe6a570751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(*parameters)\n",
    "def get_sm_prediction(accommodates, baths, bedrooms, beds, room_type, pg_type):\n",
    "    \n",
    "    const = 1\n",
    "    pr, sr = 0, 0\n",
    "    house, condo, townh, other = 0, 0, 0, 0\n",
    "    \n",
    "    if room_type == \"Private room\": pr += 1\n",
    "    else: sr += 1\n",
    "        \n",
    "    if pg_type == 'House': house += 1\n",
    "    elif pg_type == 'Condominium': condo += 1\n",
    "    elif pg_type == 'Townhouse': townh += 1\n",
    "    else: other += 1\n",
    "    \n",
    "    the_data = np.array((const, accommodates, baths, bedrooms, beds, pr, sr, house, condo, townh, other)).reshape(1, 11)\n",
    "    \n",
    "    num = lr.predict(the_data)\n",
    "    \n",
    "    return pn.indicators.Number(name=\"Statistical Modeling Prediction\", value=round(np.exp(num[0]), 3), default_color='#3B4252', \n",
    "                                font_size='60pt', title_size='50pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc4581-7266-4e94-a793-46d6dd6694d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets_col = pn.Column(accommodates, baths, bedrooms, beds, room_type, pg_type, width=500, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24019606-800a-4c9d-81e2-720e40c54ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_options = dict(align='center', sizing_mode='fixed', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e10bd9-208e-4aec-a1ac-d643b21be3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_approach = pn.Row(get_ml_prediction, widgets_col, **rows_options)\n",
    "another_approach = pn.Row(get_sm_prediction, widgets_col, **rows_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1766e0-f765-4e92-8b08-9ac81220cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = pn.pane.Markdown(\"# Predicting Airbnb Prices AU\", style={\"color\": \"#8FBCBB\"}, width=500, \n",
    "                          sizing_mode=\"stretch_width\", margin=(10,5,10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21daf6d-3e81-49a6-a2ae-9faa7fa5775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pn.pane.PNG(\"https://icons.iconarchive.com/icons/google/noto-emoji-travel-places/1024/42486-house-icon.png\", \n",
    "                 height=50, sizing_mode=\"fixed\", align=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1e4bd-f7d6-49e1-8ba2-20409506d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = pn.pane.PNG(\"https://image.flaticon.com/icons/png/512/505/505026.png\", \n",
    "                 height=50, sizing_mode=\"fixed\", align=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b60cef-ee8d-4f80-8710-2567b5f7388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = pn.Row(header, pn.Spacer(), p1, p2, background=\"#4C566A\", sizing_mode='fixed', width=1000, height=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6a102-bfd9-4346-8d9e-860b89bd1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = pn.Tabs((\"One Approach\", one_approach), (\"One Approach\", another_approach), **rows_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f6c1c-4f45-4846-b4a7-3ea19e5d4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard = pn.Column(title, tabs, background='#D8DEE9', **rows_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d8185-1cad-4df4-a4df-9bebbd10575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d6726-03f6-4122-b1a4-788c508ea95f",
   "metadata": {},
   "source": [
    "## 6. Spatial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541ac1e-f7d2-4594-8927-dce096a0bc3e",
   "metadata": {},
   "source": [
    "Accounting for the neighbourhoods in which the listings are at could provide us with a better model. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0916df-43af-4d6b-ac2a-fdceb9696ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c8c5b-8a3f-4663-a72b-26d46f01912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names.remove('constant')\n",
    "variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488dac47-d546-455f-b2a8-b6197acd2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'log_price ~ ' + ' + '.join(variable_names) + ' + neighbourhood - 1'\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3fc06-e92c-4667-87a0-3e6c8192f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = variable_names + ['neighbourhood', 'log_price']\n",
    "all_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a09896-844b-4c6a-a387-8f60d0c0af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.ols(f, data=ddf_4_modeling.loc['Austin', all_vars].compute()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3840d0-c158-4c1c-bd92-92745ef7985f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mod.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f3dc3-3777-434b-b493-94b2f5748729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_reg_scores(data, var_list):\n",
    "    return sm.ols(f, data=data[var_list]).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e40e8a-4580-46b1-a0d7-87a80417b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_models = ddf_4_modeling.map_partitions(more_reg_scores, var_list=all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e364805-1807-47ef-93db-3760a53b240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    ready_models = list(spatial_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157ef28-b841-40ba-9e26-9aa9f5e054ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ready_models[5].summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ea522-606e-44c5-90cb-6a9c6c72a7ab",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1266f-a7c6-41ca-b6b2-0015fe4ebcc6",
   "metadata": {},
   "source": [
    "In this notebook we have covered,\n",
    "\n",
    "1. Some important differences between statistical modeling and machine learning. In one, we want to predict future values as best as possible and with the other, we want to find the best model that explains the relationship between the dependent and the independent variables.\n",
    "2. You can use both kinds of models to make predictions.\n",
    "3. When possible, test your models within a quick and dirty application and observe their behavior when you change their parameters.\n",
    "4. Spatial regression can add predictive power when have that information available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
